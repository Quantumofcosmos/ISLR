---
title: "3.Linear Regression"
output: html_document
---

```{r setup, include=FALSE}
library(ISLR)
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction:

Linear regression isa very simple approach for supervised learning. In particular, it is a useful tool for predicting a quantitative response.

Consider the advertising dataset talked about in the first chapter.In order to provide recommendations, a few important questions that we might seek to answer:

* Is there a relationship between advertising budget and sales?
* How strong is the relationship between advertising budget and sales?
* Which media contribute to sales?
* How accurately can we estimate the effect of each medium on sales?
* How accurately can we predict future sales?
* Is the relationship linear?
* Is there synergy among the advertising media?

Linear regression can be used to answer each of these questions.

## Simple Linear Regression 

It assumes that relationship between $X$(predictor variable) and $Y$(quantitative response) is linear which can be written as:
$$
Y \approx \beta_0+\beta_1X\\
or\\
\hat{y}\approx\hat\beta_0+\hat\beta_1x
$$
Where $\hat\beta_0,\hat\beta_1$ are called model coefficients that are to be modeled based on available values of $x$ to predict value $\hat{y}$. Here they represent slope and intercept.

the most common approach involves minimizing the least squares or residual sum of squares.

how accurate is the sample mean $\hat{\mu}$ as an estimate of $\mu$ can be given by standard error of $\hat{\mu}$ 

$$
Var(\hat{\mu})=SE(\hat{\mu})^2=\frac{\sigma^2}{n}
$$

where $\sigma$ is the standard deviation of each of the realizations $y_i$ of Y

The estimate of $\sigma$ is known as the *residual standard error*, and is given by the formula 

$$
Residual\ standard\ error(RSE) = \sqrt{\frac{RSS}{(n − 2)}}
$$

Standard errors can be used to compute confidence intervals. A 95% confidence is defined as a range of values such that with 95 % interval probability, the range will contain the true unknown value of the parameter and can be approximately given as $\hat{\beta}\pm2.SE(\hat{\beta})$ that makes interval as $[\hat{\beta}-2.SE(\hat{\beta}),\hat{\beta}+2.SE(\hat{\beta})]$

Standard errors can also be used to perform hypothesis tests on the coefficients.

The most common **hypothesis test** involves testing the null hypothesis of $H_0$ : There is no relationship between X and Y versus the alternative hypothesis $H_a$:  There is some relationship between X and Y.

Mathematically, this corresponds to testing $H_0 : \beta_1 = 0$ versus $H_a : \beta_1 \ne 0$ since if $\beta_1=0$ then model reduces to $Y=\beta_0+\epsilon$

To test the null hypothesis, we need to determine whether $\hat{\beta_1}$, our estimate for $\beta_1$, is sufficiently far from zero, How far depends on $SE(\hat{\beta_1})$. if SE is small then relatively small values of $\hat{\beta_1}$ suggests a strong evidence to reject null hypothesis.In practice we compute **t-statistic** which measures number of standard deviations that $\hat{\beta_1}$ is away from 0. 
$$
t=\frac{\hat{\beta_1}-0}{SE(\hat{\beta_1})}
$$

### Assessing the Accuracy of the Model

Once we have rejected the null hypothesis in the favor of the alternative, next step is to quantify the extent to which the model fits the data. The quality of a linear regression fit is typically assessed using two related quantities: 
 
 * Residual standard error (RSE) 
 * R2 statistic.

#### Residual standard error (RSE)

Due to the term $\epsilon$ always present even in a true regression line we would not be able to predict $Y$ from a given $X$. RSE is an estimate of the standard deviation of $\epsilon$.Roughly speaking, it is the average amount that the response will deviate from the true regression line.

$$
RSE = \sqrt{\frac{1}{n-2}RSS} = \sqrt{\frac{1}{n-2}\sum_{i=1}^n(y_i-\hat{y_i})^2}
$$
If RSE is small then $\hat{y_i}\approx y_i$ indicating that the model fits the data well.


#### $R^2$ Statistic

> The RSE provides an absolute measure of lack of fit of the model to the data. But since it is measured in the units of Y , it is not always clear what constitutes a good RSE. The $R^2$ statistic provides an alternative measure of fit. It takes the form of a proportion—the proportion of variance explained—and so it always takes on a value between 0 and 1, and is independent of the scale of Y .

$$
R^2 = \frac{TSS-RSS}{TSS} = 1-\frac{RSS}{TSS} = 1-\frac{\sum_{i=1}^n(y_i-\hat{y_i})^2}{\sum(y_i-\bar{y})^2}
$$

 >TSS measures the total variance in the response Y , and can be squares thought of as the amount of variability inherent in the response before the regression is performed. In contrast, RSS measures the amount of variability that is left unexplained after performing the regression.

 > $R^2$ measures the proportion of variability in Y that can be explained using X

An $R^2$ statistic that is close to 1 indicates that a large proportion of the variability in the response has been explained by the regression. A number near 0 indicates that the regression did not explain much of the variability in the response; this might occur because the linear model is wrong, or the inherent error σ2 is high, or both.

An $R^2$ statistic has an interpretational advantage over RSE since it always lies between 0 and 1.Recall that correlation is also a measure of the linear relationship between X and Y and it can be shown that in the simple linear regression setting, $R^2 = (Cor(X,Y))^2$. However correlation quickly falls apart when dealing with multiple linear regression and this role is filled by $R^2$.

## Multiple Linear Regression

Simple linear regression is useful when prediction is based on a single predictor variable.However in practice we have more than one predictor. 

One option to extend the analysis to accommodate two additional predictors is to run three separate regressions. However this option is not entirely satisfactory as the regression equation ignores other variables when modeling one and also it cannot give one prediction value for given multiple predictors.

 >Instead of fitting a separate simple linear regression model for each predictor, a better approach is to extend the simple linear regression model so that it can directly accommodate multiple predictors. We can do this by giving each predictor a separate slope coefficient in a single model. In general, suppose that we have p distinct predictors. Then the multiple linear regression model takes the form

$$
Y=\beta_0+\beta_1X_1+\beta_2X_2+ \ .\ .\ .+\beta_pX_p+\epsilon 
$$
where $X_j$ represents the $jth$ predictor and $\beta_j$ quantifies the association between that variable and the response. We interpret $\beta_j$ as *average effect* on $Y$ of one unit change in $X_j$, holding all other predictors fixed.

### Estimating the Regression Coefficients

As was in the case of simple linear regression, coefficients $\beta_0,\beta_0,\ .\ .\ .,\beta_p $ are unknown and must be estimated using least squares approach. 

### Some important questions

When performing multiple linear regression, we usually are interested in answering a few important questions

 * Is at least one of the predictors $X_1, X_2,...,X_p$ useful in predicting the response?
 * Do all the predictors help to explain Y , or is only a subset of the predictors useful?
 * How well does the model fit the data?
 * Given a set of predictor values, what response value should we predict, and how accurate is our prediction?

#### Is There a Relationship Between the Response and Predictors?

87



















83  88  93  98 103 108 113 118 123 128 133 138 143 148 153 158